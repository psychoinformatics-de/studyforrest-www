<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="StudyForrest Project" />
  <title>studyforrest.org &mdash; Overview of Data Types</title>
  <link rel="license" hreflang="en" href="/copyright.html" />

  <!-- favicons were once a simple race, but they dug too greedily and too deep... -->
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=5">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=5">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=5">
  <link rel="manifest" href="/site.webmanifest?v=5">
  <link rel="mask-icon" href="/safari-pinned-tab.svg?v=5" color="#5bbad5">
  <link rel="shortcut icon" href="/favicon.ico?v=5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />
</head>
<body>
  <nav>
    <div>
      <div>
        <h1><a href='/'><span class="icon-studyforrest"></span>StudyForrest</a></h1>
        <ul>
          <li><a href="about.html" >About</a></li>
          <li><a href="access.html" >Access</a></li>
          <li><a href="data.html" class='active'>Data</a></li>
          <li><a href="explore.html" >Explore</a></li>
          <li><a href="publications.html" >Publications</a></li>
        </ul>
      </div>
      <form action="https://www.studyforrest.org/search.html" role="search">
        <input id="tipue_search_input" aria-label="Search" type="search" placeholder="Search" name="q"><button type="submit" aria-label='Search'><span class="icon-search"></span></button>
      </form>
    </div>
  </nav>


  <main>
    <div id='content'>
<article>
  <header>
    <h1>Overview of Data Types</h1>
  </header>
  
  <p>Since its inception, the <em>StudyForrest</em> dataset has grown
     appreciably through both internal and external contributions. Made
     available are raw data, preprocessed data, and an extensive collection of
     annotations of the movie. Often, multiple data types are available for the
     same set of participants &mdash; for example, multiple fMRI scans of up to
     10 hours total per participant.</p>

  <p>Below is an overview of all major components of the <em>StudyForrest</em>
     dataset. This includes data relating to brain structure, brain function,
     and movie stimulus properties. Further details for each data type can be
     found in the linked publications.</p>

  <form action="javascript:void(0)" onsubmit='filterForm(document.getElementById("data_filter_field").value);' onreset='filterForm("")'>
    <input id="data_filter_field" type="text" placeholder="Filter data types for..." aria-label="Filter data types for..." /><button title="Filter Table" type="submit"><span class="icon-filter"></span></button>
    <input type="reset" value="Reset">
  </form>
  <noscript>
    <p><strong>Enable JavaScript to filter through data types.</strong></p>
  </noscript>

  <section class="data-category">
    <h2>Behavior and Brain Function</h2>
    <p class="data-category-count"></p>

    <p>A diverse set of stimulation paradigms and data acquisition setups were
    utilized to characterize participant's brain function on a variety of
    dimensions.</p>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_moviefmri_acq_ao_hrfmri.jpg" alt="Person hearing a movie, with recording of brainfunction,
             heartbeat, and breathing." />
      </div>
      <div class="description">
        <h3>High-res 7T fMRI on 2h Audio Movie (+Cardiac/Respiration)</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>7T, audio, cardiac, respiration
        </p>
        <p>Two principle data modalities were acquired: Blood oxygenation level
           dependent (BOLD) fMRI scans, continuously capturing brain activity at
           a spatial resolution of 1.4 mm, and physiological recordings of heart
           beat and breathing. For technical validation, all measurements are
           also available for a full-length gel phantom scan.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_musicfmri_acq.jpg" alt="Person hearing music, with recording of brainfunction,
             heartbeat, and breathing." />
      </div>
      <div class="description">
        <h3>High-res 7T fMRI Listening to Music (+Cardiac/Respiration)</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.6679.1">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>7T, music, cardiac, respiration
        </p>
        <p>High-resolution, ultra high-field (7 Tesla) functional magnetic
           resonance imaging (fMRI) data from 20 participants that were
           repeatedly stimulated with a total of 25 music clips, with and
           without speech content, from five different genres using a slow
           event-related paradigm. Physiological recordings of heart beat and
           breathing were simultaneously recorded as well.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_moviefmri_acq_av_lrfmri_eyegaze.jpg" alt="Person watching and hearing a movie, with recording of
             brainfunction, heartbeat, and breathing." />
      </div>
      <div class="description">
        <h3>3T fMRI on 2h Movie, Eyegaze (+Cardiac/Respiration)</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2016.92">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>3T, audio, eyegaze, cardiac, respiration
        </p>
        <p>Two-hour 3 Tesla fMRI acquisition while 15 participants were shown an
           audio-visual version of the stimulus motion picture, simultaneously
           recording eye gaze location, heart beat, and breathing.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_retmap_acq.jpg" alt="Retinotopic Map" />
      </div>
      <div class="description">
        <h3>Retinotopic Mapping</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2016.93">Publication</a>
           <span class="icon-tags"></span>3T, retinotopic mapping, visual
           cortex, eccentricity, polar angle
        </p>
        <p>Standard 3 mm fMRI recording of a retinotopic mapping procedure with
           expanding and contracting ring and rotating wedge stimuli. Resulting
           eccentricity and polar angle maps of the visual cortex of 15
           participants are available.
        </p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_visloc_acq.jpg" alt="Brain with person, face, stapler, and house" />
      </div>
      <div class="description">
        <h3>Higher Visual Area Localizer</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2016.93">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>3T, visual area localizer, block-design, one-back task
        </p>
        <p>3mm fMRI data from a standard block-design visual area localizer
           using greyscale images for the stimulus categories human faces, human
           bodies without heads, small objects, houses and outdoor scenes
           comprising of nature and street scenes, and phase scrambled images.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_orientfmri_acq.jpg" alt="Sample visual stimulus with high contrast parallel lines" />
      </div>
      <div class="description">
        <h3>Multi-res 3T/7T fMRI (0.8-3mm) on Visual Orientation</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1016/j.neuroimage.2016.12.040">Publication</a>
           <span class="icon-tags"></span>7T, 3T, visual, oriented gratings, decoding
        </p>
        <p>Ultra high-field 7T and 3T fMRI data recorded at 0.8 (7T-only), 1.4,
           2, and 3 mm isotropic voxel size under stimulation with flickering,
           oriented grating stimuli. Grating orientation in the left and right
           visual field varied independently to enable decoding analyses.</p>
      </div>
    </div>
  </section>

  <section class="data-category">
    <h2>Brain Structure and Connectivity</h2>
    <p class="data-category-count"></p>

    <p>A versatile set of structural brain images are available to provide a
       comprehensive in-vivo assessment of all participants' brain hardware.</p>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_t1w.jpg" alt="T1 brain image" />
      </div>
      <div class="description">
        <h3>T1-weighted MRI</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-tags"></span>3T, T1
        </p>
        <p>An image with 274 sagittal slices (FoV 191.8×256×256 mm) and an
           acquisition voxel size of 0.7 mm with a 384×384 in-plane
           reconstruction matrix (0.67 mm isotropic resolution) was recorded
           using a 3D turbo field echo (TFE) sequence (TR 2500 ms, inversion
           time (TI) 900 ms, flip angle 8 degrees, echo time (TE) 5.7 ms,
           bandwidth 144.4 Hz/px, Sense reduction AP 1.2, RL 2.0, scan duration
           12:49 min).</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_t2w.jpg" alt="T2 brain image" />
      </div>
      <div class="description">
        <h3>T2-weighted MRI</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-tags"></span>3T, T2
        </p>
        <p>A 3D turbo spin-echo (TSE) sequence (TR 2500 ms, TEeff 230 ms, strong
           SPIR fat suppression, TSE factor 105, bandwidth 744.8 Hz/px, Sense
           reduction AP 2.0, RL 2.0, scan duration 7:40 min) was used to acquire
           an image whose geometric properties otherwise match the T1-weighted
           image.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_swi.jpg" alt="SWI brain image" />
      </div>
      <div class="description">
        <h3>Susceptibility-weighted MRI</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-tags"></span>3T, SWI
        </p>
        <p>An image with 500 axial slices (thickness 0.35 mm, FoV 181×202×175 mm)
           and an in-plane acquisition voxel size of 0.7 mm reconstructed at
           0.43 mm (512×512 matrix) was recorded using a 3D Presto fast field
           echo (FFE) sequence (TR 19 ms, TE shifted 26 ms, flip angle 10
           degrees, bandwidth 217.2 Hz/px, NSA 2, Sense reduction AP 2.5, FH
           2.0, scan duration 7:13 min).</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_dti.jpg" alt="DTI brain image" />
      </div>
      <div class="description">
        <h3>Diffusion-weighted MRI</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>3T, DTI
        </p>
        <p>Diffusion data were recorded with a diffusion-weighted single-shot
           spin-echo EPI sequence (TR 9545 ms, TE 80 ms, strong SPIR fat
           suppression, bandwidth 2058.4 Hz/px, Sense reduction AP 2.0) using 32
           diffusion-sensitizing gradient directions with b=800smm2b=800smm2
           (two samples for each direction), 70 slices (thickness of 2 mm and an
           in-plane acquisition voxel size of 2×2 mm, reconstructed at
           1.7×1.7 mm, 144×144 in-plane matrix, FoV 224×248×140 mm). Acquisition
           duration was 12:38 min.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_angio.jpg" alt="Angiography" />
      </div>
      <div class="description">
        <h3>Angiography</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-tags"></span>7T, angiography
        </p>
        <p>A 3D multi-slab time-of-flight angiography was recorded at 7 Tesla
           for the FoV of the fMRI recording. Four slabs with 52 slices
           (thickness 0.3 mm) each were recorded (192×144 mm FoV, in-plane
           resolution 0.3×0.3 mm, GRAPPA acceleration factor 2, phase encoding
           direction right-to-left, 15.4% slice oversampling, 24 ms TR, 3.84 ms
           TE).</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_surf.jpg" alt="Closeup of FreeSurfer's cortical surface reconstruction" />
      </div>
      <div class="description">
        <h3>Cortical Surface Reconstruction</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.1038/sdata.2014.3">Publication</a>
           <span class="icon-flashlight"></span><a href="/explore.html">Explore</a>
           <span class="icon-tags"></span>derivative
        </p>
        <p>Reconstructed cortical surfaces meshes and various associated
           estimates were generated from the T1 and T2-weighted images of each
           participant using the FreeSurfer 5.3 image segmentation and
           reconstruction pipeline.</p>
      </div>
    </div>
  </section>

  <section class="data-category">
    <h2>Movie Stimulus Annotations</h2>
    <p class="data-category-count"></p>
    <p>Annotating the content of the Forrest Gump movie, the key stimulus used
       in the project, is an open-ended endeavour. Its naturalistic nature is
       rich in diverse visual and auditory features, but also the facets of
       social communication enables and requires a versatile description.</p>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_structure.png" alt="Multi-track representation of scenes, segments, shots, and MRI
             segments." />
      </div>
      <div class="description">
        <h3>Location Changes and Time Progression</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.9536.1">Publication</a>
           <span class="icon-tags"></span>movie, annotation, cut, scenes, time progression, location
        </p>
        <p>Start and end times for all scenes in the cut of the movie that is
           used as the stimulus. In addition, each annotation contains whether a
           scene takes place indoors or outdoors. Additionally, temporal
           location of all cuts in the movie (only applicable to visual
           stimulation). Cuts were initially detected using an automated
           procedure and were then later curated by hand.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_speech.jpg" alt="Forrest shaking the hand of Lt. Dan. Captioned." />
      </div>
      <div class="description">
        <h3>Speech</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.27621.1">Publication</a>
           <span class="icon-tags"></span>movie, annotation, speech, grammar, word, dialog
        </p>
        <p>The exact timing of each of the more than 2,500 spoken sentences,
           16,000 words (including 202 non-speech vocalizations), 66,000
           phonemes, and their corresponding speaker. Additionally, every word
           is associated with a grammatical category, and its syntactic
           dependencies are defined.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_emotion.jpg" alt="Closeup on the face of an excited man." />
      </div>
      <div class="description">
        <h3>Portrayed Emotions</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.6230.1">Publication</a>
           <span class="icon-tags"></span>movie, annotation, emotion, arousal, valence, cue
        </p>
        <p>Description of portrayed emotions in the movie and the audio
           description stimulus. The nature of an emotion is characterized with
           basic attributes such as onset, duration, arousal, and valence
           &mdash; as well as explicit emotion category labels and a record of
           the perceptual evidence for the presence of an emotion.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_irony.jpg" alt="A sign saying 'Hidden Beach'; CC-BY from https://www.flickr.com/photos/carbonnyc/76468122" />
      </div>
      <div class="description">
        <h3>Semantic Conflict</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.9635.1">Publication</a>
           <span class="icon-tags"></span>movie, annotation, lies, irony
        </p>
        <p>Identification of episodes with portrayal of lies, irony, or sarcasm
           by three independent observers.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_locationtime.jpg" alt="Four shots from the same scene, but in different locations." />
      </div>
      <div class="description">
        <h3>Location Changes and Time Progression</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.12688/f1000research.9536.1">Publication</a>
           <span class="icon-tags"></span>movie, annotation
        </p>
        <p>An annotation of location and temporal progression in the movie.</p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_bodycontact.jpg" alt="Young Forrest and young Jenny hold hands." />
      </div>
      <div class="description">
        <h3>Body Contact</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://github.com/psychoinformatics-de/studyforrest-paper-bodycontactannotation/blob/master/paper/p.tex">Description</a>
           <span class="icon-tags"></span>movie, annotation, body parts, touch, body language
        </p>
        <p>A detailed description of all body contact events in the movie,
           including timing, actor and recipient, body parts involved, intensity
           and valence of the touch, and any potential audio cues.
        </p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_eyegaze.jpg" alt="Forrest pointing to flashlights in the Watergate building,
             with eyetracking data overlaid." />
      </div>
      <div class="description">
        <h3>Eye Movement Labels</h3>
        <p class="info">
           <span class="icon-doc-text"></span>
           <a href="https://doi.org/10.3758/s13428-020-01428-x">Publication</a>
           <span class="icon-tags"></span>movie, annotation, eye gaze, saccades, fixations, smooth pursuit
        </p>
        <p>Classification of eye movements for two groups of 15 participants
           watching the movie: one group inside an MRI scanner and another
           group in a laboratory setting. Saccades, post-saccadic oscillations,
           fixations, and smooth pursuit events are distinguished.
        </p>
      </div>
    </div>

    <div class="card-data">
      <div class="thumbnail">
        <img src="/img/data/thumb_annot_music.jpg" alt='The word "Music", stylized. CC-BY-SA https://commons.wikimedia.org/wiki/File:Maroper_Music.jpg' />
      </div>
      <div class="description">
        <h3>Music</h3>
        <p class="info">
          <span class="icon-tags"></span>music, annotations, soundtrack, music</p>
        <p>Timing and identity of every musical piece in the movie's soundtrack.</p>
      </div>
    </div>
  </section>

  <h2>Participant/Acquisition Summary</h2>
  <p>The following table shows what data are available for each participant.
     Participant IDs are consistent across all acquisitions.</p>

  <p>Raw and preprocessed data were released over time as several datasets, and
     are typically available from multiple locations. If you cannot locate a
     dataset component you are interested in, please get in touch. Likewise, if
     you want to re-share data that was preprocessed in a particular way, please
     contact
     <a href="mailto:info@studyforrest.org?subject=studyforrest.org">info@studyforrest.org.</a></p>

  <table class="participant-summary">
    <thead>
      <tr>
        <th scope="row">Participant ID</th>
        <th scope="col">1-3</th>
        <th scope="col">4</th>
        <th scope="col">5</th>
        <th scope="col">6</th>
        <th scope="col">7-8</th>
        <th scope="col">9-10</th>
        <th scope="col">11-13</th>
        <th scope="col">14-15</th>
        <th scope="col">16-17</th>
        <th scope="col">18</th>
        <th scope="col">19</th>
        <th scope="col">20</th>
        <th scope="col">21</th>
        <th scope="col">22-36</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th scope="rowgroup" colspan="15">Structural MRI</th>
      </tr>
      <tr><th scope="row">T1, T2, DWI, SWI, Angiography</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
      </tr>
    </tbody>
    <tbody>
      <tr>
        <th scope="rowgroup" colspan="15">Natural Stimulation</th>
      </tr>
      <tr><th scope="row">2h audio movie (7T, +cardiac/resp)</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">2h audio-visual movie (3T, eyegaze, +cardiac/resp)</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">2h audio-visual movie (in-lab eyetracking)</th>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td>&check;</td>
      </tr>
    </tbody>
    <tbody>
      <tr>
        <th scope="rowgroup" colspan="15">Task fMRI</th>
      </tr>
      <tr><th scope="row">Listening to music (7T, +cardiac/resp)</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">Retinotopic mapping (3T)</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
      </tr>
      <tr><th scope="row">Visual area localizer (3T)</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">Flickering oriented gratings (7T @ 0.8, 1.4, 2, and 3mm)</th>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
      </tr>
      <tr><th scope="row">Flickering oriented gratings (3T @ 1.4, 2, and 3mm)</th>
        <td> </td>
        <td> </td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
      </tr>
    </tbody>
    <tbody>
      <tr>
        <th scope="rowgroup" colspan="15">Preprocessed Data</th>
      </tr>
      <tr><th scope="row">FreeSurfer cortical surface reconstruction</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">Participant and scan-specific template MRI images (for
              alignment, masking, structural properties) &amp; (non-)linear
              transformations between image spaces</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">Per-participant aligned fMRI data for within-subject analysis</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
      <tr><th scope="row">Audio-visual movie fMRI aggregate ROI timeseries for
              Shen et al. (2013) cortex parcellation</th>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td>&check;</td>
        <td> </td>
        <td> </td>
      </tr>
    </tbody>
  </table>
  <script>
    var data_categories = document.getElementsByClassName('data-category');
    var data_filter     = document.getElementById('data_filter_field');

    // check if there's a term to filter from the URL
    var term = getURLParam('filter');
    if (term) {
      data_filter.value = term;
      filterDataTypes(term);
    }

    function filterForm(term) {
      filterDataTypes(term);
      var history_url = '';
      var history_title = '';

      if (!term || term.length === 0) {
        history_url = location.href.split('?')[0];
      } else {
        history_url = history_url + '?filter=' + term;
        history_title = 'Data - ' + term;
      }

      // add to address bar and history
      history.pushState({}, history_title, history_url);
      return false;
    }

    function getURLParam(param) {
      var val = '';
      location.search.substr(1)
        .split("&")
        .some(function(item) {
          return item.split("=")[0] == param && (val = item.split("=")[1])
        });
      return val;
    }

    function filterDataTypes(term) {
      term = term.trim();

      for (var i=0; i < data_categories.length; i++) {
        var category = data_categories[i];
        var category_count = category.querySelector('.data-category-count');
        var category_cards = category.getElementsByClassName('card-data');
        var category_total = category_cards.length;
        var category_shown = category_total;

        for (var j=0; j < category_cards.length; j++) {
          if (category_cards[j].textContent.toLowerCase().indexOf(term.toLowerCase()) == -1) {
            category_cards[j].style.display = 'none';
            category_shown--;
          } else {
            category_cards[j].style.display = '';
          }
        }

        if (!term || term.length === 0) {
          category_count.innerHTML = '';
        } else {
          category_count.innerHTML = '<strong>Showing ' + category_shown + ' of ' + category_total + ' data types containing \'' + term + '\'</strong>.';
        }
      }
    }
  </script>

</article>
    </div><!-- /.content -->
  </main>

  <footer>
    <div>
      <div class='left'>
        <p>Website sources are available <a href="https://github.com/psychoinformatics-de/studyforrest-www">on GitHub</a>.</p>
        <p>Content <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
           unless <a rel="license" href='/copyright.html'>indicated otherwise</a>.</p>
        <p>&copy; 2014–2021 StudyForrest Project</p>
      </div>

      <span class="icon-studyforrest center"></span>

      <div class='right'>
        <p><strong>Contact Us:</strong></p>
        <p><a href="mailto:info@studyforrest.org">info@studyforrest.org</a></p>
        <ul class='social-links'>
          <li><a class="icon-github" href='https://github.com/psychoinformatics-de?q=studyforrest' aria-label='StudyForrest @ GitHub'></a></li>
          <li><a class="icon-twitter" href='https://twitter.com/studyforrest' aria-label='StudyForrest @ Twitter'></a></li>
        </ul>
      </div>
    </div>
  </footer>
</body>
</html>